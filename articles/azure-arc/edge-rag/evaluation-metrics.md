---

title: Metrics for Evaluating the Edge RAG System
description: "Learn how to evaluate the Edge RAG system using metrics like correctness, relevancy, precision, and recall for optimal performance."
author: cwatson-cat
ms.author: cwatson
ms.topic: reference #Don't change
ms.date: 05/13/2025
ai-usage: ai-assisted
ms.subservice: edge-rag
ms.custom:
  - build-2025
# Customer Intent:  As a developer or data scientist, I want to understand the metrics for evaluating the Edge RAG system so that I can assess the quality and performance of generated responses and retrieval processes effectively.
---

# Metrics for evaluating the Edge RAG Preview system

This article lists the metrics used when you evaluate the system of Edge RAG Preview, enabled by Azure Arc. For more information, see [Evaluate the Edge RAG system](evaluate-solution.md)

[!INCLUDE [preview-notice](includes/preview-notice.md)]

## Generation metrics

The following metrics for evaluate the quality of generated responses.

| Metric         |Description               |
|----------------|-------|
|Correctness |Evaluates the accuracy and factual validity of generated responses against the expected responses (ground truth). </br></br>Range score: 1-5|
| Groundedness   | Evaluates the degree to which the responses generated by the generative AI application correspond with the information provided from the retrieved documents. </br></br>Range score: 1-5         |
| Relevancy      | Evaluates the degree to which the responses generated by the generative AI application are appropriate and directly correspond to the provided input. </br></br>Range score: 1-5|
| Rouge L        | Measures the longest common subsequence between the generated text and reference text. </br></br>Range score: 0-1 |
| Bleu           | Evaluates the quality of generated text by comparing it to expected responses (ground truth) while penalizing on the brevity.  </br></br>Range score: 0-1        |
| Meteor         | METEOR (Metric for Evaluation of Translation with Explicit Ordering) evaluates the quality of generated text by comparing it to expected responses (ground truth) while penalizing on misalignment in fragments of the actual vs. expected sentences. </br></br>Range score: 0-1    |

## Information retrieval metrics

The following metrics for evaluate the retrieval performance.

| Metric         |Description         |
|----------------|-------|
| Precision      | Measures the proportion of correctly retrieved documents among all retrieved document. </br></br>Range score: 0-1|
| Recall         | Measures the proportion of retrieved documents among all relevant documents. </br></br>Range score: 0-1         |
| MRR            | Mean reciprocal rank (MRR) measures the quality of document ranking based on the position of the first relevant document. </br></br>Range score: 0-1     |

## Related content

[Evaluate the Edge RAG system](evaluate-solution.md)
